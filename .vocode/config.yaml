# Default Vocode configuration
ui:
#  log_level: debug
variables:
  LLM_DISCOVERY_MODEL: openai/gpt-5-mini
  LLM_ARCHITECT_MODEL: openai/gpt-5
  LLM_CODER_MODEL: openai/gpt-5-mini
#  LLM_DISCOVERY_MODEL: gemini/gemini-2.5-flash
#  LLM_ARCHITECT_MODEL: gemini/gemini-2.5-pro
#  LLM_CODER_MODEL: gemini/gemini-2.5-flash
#  LLM_DIFF_FORMAT: patch
tools:
  - name: vectorops_read_files
    auto_approve: true
  - name: vectorops_summarize_files
    auto_approve: true
  - name: vectorops_list_files
    auto_approve: true
  - name: vectorops_search
    auto_approve: true
workflows:
  architect:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        outcomes:
          - name: done
      - $include:
          template: nodes/discovery.yaml
        function_tokens_pct: 25
        confirmation: auto
      - $include:
          template: nodes/architect.yaml
        tools:
          - vectorops_summarize_files
          - vectorops_read_files
        preprocessors:
          - name: file_read
            paths:
              - AGENTS.md
        function_tokens_pct: 25
        reset_policy: keep_results
      - $include:
          template: nodes/coder.yaml
        tools:
          - vectorops_read_files
        confirmation: auto
      - $include:
          template: nodes/apply_patch.yaml
        confirmation: auto
      - name: test
        type: exec
        command: uv run pytest -q -rf ./tests/
        timeout_s: 300
        expected_return_code: 0
        message: "All changes were successfully applied. Unit test results are below:"
        confirmation: auto
        outcomes:
          - name: success
          - name: fail
    edges:
      - prompt.done -> discovery
      - discovery.done -> architect
      - architect.done -> coder
      - coder.done -> apply
      - apply.success -> test
      - apply.fail -> coder:keep_state
      - test.success -> prompt
      - test.fail -> architect:keep_state
  diff:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        outcomes:
          - name: done
      - $include:
          template: nodes/architect.yaml
        model: gpt-5-mini
      - name: finish
        type: noop
    edges:
      - prompt.done -> architect
      - architect.done -> finish
  # Aider-like explicit coder loop
  coder:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        outcomes:
          - name: done
      - $include:
          template: nodes/file_state.yaml
          skip: true
      - $include:
          template: nodes/architect.yaml
          reset_policy: keep_state
        preprocessors:
          - name: file_state
            mode: user
          - name: file_read
            paths:
              - AGENTS.md
      - $include:
          template: nodes/coder.yaml
        confirmation: auto
      - $include:
          template: nodes/apply_patch.yaml
        confirmation: auto
      - name: test
        type: exec
        command: uv run pytest -q -rf ./tests/
        timeout_s: 300
        expected_return_code: 0
        message: "Tests results below:"
        confirmation: auto
        outcomes:
          - name: success
          - name: fail
    edges:
      - prompt.done -> architect
      - architect.done -> coder
      - coder.done -> apply
      - apply.success -> test
      - apply.fail -> coder:keep_state
      - test.success -> prompt
      - test.fail -> architect:keep_state
  # Aider-like explicit coder loop
  ask:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        outcomes:
          - name: done
      - $include:
          template: nodes/analyst.yaml
        tools:
          - vectorops_search
          - vectorops_summarize_files
          - vectorops_read_files
    edges:
      - prompt.done -> analyst
      - analyst.done -> prompt
  idler:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        outcomes:
          - name: done
      - name: pause
        type: noop
        sleep_seconds: 5
        outcomes:
          - name: done
      - name: debug
        type: debug
        outcomes:
          - name: done
      - name: done
        type: noop
        confirmation: auto
    edges:
      - prompt.done -> pause
      - pause.done -> debug
      - debug.done -> done
  patcher:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        outcomes:
          - name: done
      - $include:
          template: nodes/coder.yaml
      - $include:
          template: nodes/apply_patch.yaml
        confirmation: auto
      - name: finish
        type: noop
    edges:
      - prompt.done -> coder
      - coder.done -> apply
      - apply.success -> prompt
      - apply.fail -> coder:keep_state
  apply:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        outcomes:
          - name: done
      - $include:
          template: nodes/apply_patch.yaml
        confirmation: auto
      - name: finish
        type: noop
    edges:
      - prompt.done -> apply
      - apply.success -> prompt
      - apply.fail -> prompt
  chat:
    nodes:
      - name: prompt
        type: input
        message: Enter your request
        confirmation: auto
        hide_final_output: true
        system: |
          You're friendly chat assistant. Take user prompts and provide necessary answers.

          * Analyze existing project by calling discovery tools before providing an anwer.
        outcomes:
          - name: done
      - name: llm
        type: llm
        model: ${LLM_ARCHITECT_MODEL}
        reset_policy: keep_state
        tools:
          - vectorops_search
          - vectorops_summarize_files
          - vectorops_read_files
        outcomes:
          - name: done
    edges:
      - prompt.done -> llm
      - llm.done -> prompt
